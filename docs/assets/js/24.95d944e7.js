(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{384:function(a,e,t){"use strict";t.r(e);var s=t(46),r=Object(s.a)({},(function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[a._v("#")]),a._v(" Introduction")]),a._v(" "),t("p",[a._v("DataYoga is a framwork for building and generating data pipelines. The DataYoga CLI helps define data pipelines using a semantic markup language using yaml files. These pipeline definitions are then used to generate executable artifacts running on a variety of Processing engines such as PySpark.")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/datayoga-io/datayoga/blob/main/docs/datayoga_concept.png?raw=true",alt:"DataYoga concept"}})]),a._v(" "),t("h1",{attrs:{id:"data-entities"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#data-entities"}},[a._v("#")]),a._v(" Data Entities")]),a._v(" "),t("p",[a._v("DataYoga models the data ecosystem using the following entities:")]),a._v(" "),t("p",[t("code",[a._v("Datastore")]),a._v(" - A datastore represents a source or target of data that can hold data at rest or data in motion. Datastores include entities such as a table in a database, a file, or a stream. A Datastore can act either as a source or a target of a pipeline.")]),a._v(" "),t("p",[t("code",[a._v("File")]),a._v(" - A file is a type of Datastore that represents information stored in files. Files contain metadata about their structure and schema.")]),a._v(" "),t("p",[t("code",[a._v("Dimension")]),a._v(" - A dimension table / file is typically used for lookup and constant information that is managed as part of the application code. This often includes lookup values such as country codes.")]),a._v(" "),t("p",[t("code",[a._v("Runner")]),a._v(" - A runner is an executable capable of running code. The "),t("code",[a._v("Runner")]),a._v(" communicates with a "),t("code",[a._v("Processor")]),a._v(" to execute the code. A "),t("code",[a._v("Runner")]),a._v(" can be a local NodeJs process running queries in a database (a "),t("code",[a._v("Processor")]),a._v("), or PySpark which acts both as a "),t("code",[a._v("Runner")]),a._v(" and a "),t("code",[a._v("Processor")]),a._v(".")]),a._v(" "),t("p",[t("code",[a._v("Processor")]),a._v(" - A processing engine capable of running data operations. Every "),t("code",[a._v("Processor")]),a._v(" supports one or more programming languages. Some "),t("code",[a._v("Processors")]),a._v(", like a database engine, may only support SQL, while others like Spark may support Python, Scala, and Java.")]),a._v(" "),t("p",[t("code",[a._v("Consumer")]),a._v(" - A consumer consumes data and presents it to a user. Consumers include reports, dashboards, and interactive applications.")]),a._v(" "),t("p",[t("code",[a._v("Pipeline")]),a._v(" - A pipeline represents a series of "),t("code",[a._v("Jobs")]),a._v(" that operate on a single "),t("code",[a._v("Runner")]),a._v(".")]),a._v(" "),t("p",[t("code",[a._v("Job")]),a._v(" - A job is composed of a series of Steps that fetch information from one or more Datastores, transform them, and store the result in a target Datastore, or perform actions such as sending out alerts or performing HTTP calls.")]),a._v(" "),t("p",[t("code",[a._v("Job Step")]),a._v(" - Every step in a job performs a single action. A step can be of a certain "),t("em",[a._v("type")]),a._v(" representing the action it performs. A step can be an SQL statement, a Python statement, or a callout to a library. Steps can be chained to create a Directed Acyclic Graph (DAG).")]),a._v(" "),t("h1",{attrs:{id:"getting-started"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#getting-started"}},[a._v("#")]),a._v(" Getting started")]),a._v(" "),t("h2",{attrs:{id:"pre-requisites"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pre-requisites"}},[a._v("#")]),a._v(" Pre-requisites")]),a._v(" "),t("h3",{attrs:{id:"install-nodejs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#install-nodejs"}},[a._v("#")]),a._v(" Install NodeJS")]),a._v(" "),t("p",[a._v("https://nodejs.org/en/download/package-manager/")]),a._v(" "),t("h2",{attrs:{id:"installing-the-cli"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#installing-the-cli"}},[a._v("#")]),a._v(" Installing the CLI")]),a._v(" "),t("p",[a._v("Install the CLI")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("npm install -g @datayoga-io/datayoga\n")])])]),t("p",[a._v("Verify that the installation completed successfully by running the following command:")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("dy-cli --version\n")])])]),t("h2",{attrs:{id:"create-a-new-datayoga-project"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#create-a-new-datayoga-project"}},[a._v("#")]),a._v(" Create a new datayoga project")]),a._v(" "),t("p",[a._v("To create a new datayoga project, use the "),t("code",[a._v("init")]),a._v(" command.")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("dy-cli init myproject\ncd myproject\n")])])]),t("p",[a._v("You will see a folder structure that scaffolds a new datayoga enviornment. The scaffold also includes a demo northwind sqlite database and sample pipelines.")]),a._v(" "),t("h2",{attrs:{id:"validating-the-install"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#validating-the-install"}},[a._v("#")]),a._v(" Validating the install")]),a._v(" "),t("p",[a._v("Let's run our first job. It is pre-defined in the samples folder as part of the "),t("code",[a._v("init")]),a._v(" command.")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("dy-cli run sample.customer\n")])])]),t("p",[a._v("That's it! You've created your first job that loads data from CSV, runs it through a basic transformation, and upserts the data into a target table.")]),a._v(" "),t("p",[a._v("Read the "),t("a",{attrs:{href:"https://datayoga.io/docs/guide/",target:"_blank",rel:"noopener noreferrer"}},[a._v("guide"),t("OutboundLink")],1),a._v(" for a more detailed tutorial or check out the "),t("a",{attrs:{href:"https://datayoga.io/docs/reference/CLI.html",target:"_blank",rel:"noopener noreferrer"}},[a._v("reference"),t("OutboundLink")],1),a._v(" to see various blocks types currently available.")]),a._v(" "),t("h2",{attrs:{id:"using-the-spark-runner"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#using-the-spark-runner"}},[a._v("#")]),a._v(" Using the Spark runner")]),a._v(" "),t("p",[a._v("In order to run a Job as an ETL (Extract-Transform-Load) job, DataYoga support Spark.")]),a._v(" "),t("h3",{attrs:{id:"install-local-spark-runner"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#install-local-spark-runner"}},[a._v("#")]),a._v(" Install local Spark runner")]),a._v(" "),t("p",[a._v("To run jobs locally, datayoga uses data processing technologies called "),t("code",[a._v("runners")]),a._v(". We provide a packaged docker container with a pre-installed Spark runner.")]),a._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[a._v("docker run -it --name dy-spark-runner --add-host host.docker.internal:host-gateway -p "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("8998")]),a._v(":8998 -p "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("8000")]),a._v(":8000 -v "),t("span",{pre:!0,attrs:{class:"token variable"}},[t("span",{pre:!0,attrs:{class:"token variable"}},[a._v("$(")]),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("pwd")]),t("span",{pre:!0,attrs:{class:"token variable"}},[a._v(")")])]),a._v("/data:/opt/dy/data datayoga/dy-runner-spark:latest\n")])])]),t("div",{staticClass:"custom-block warning"},[t("p",{staticClass:"custom-block-title"},[a._v("Note")]),a._v(" "),t("p",[a._v("We are mapping the volume of "),t("code",[a._v("/opt/dy/data")]),a._v(" to the folder named "),t("code",[a._v("data")]),a._v(". If you are running this from the datayoga project home folder, this should have been created as part of the "),t("code",[a._v("init")]),a._v(" command. You can point this volume to any local folder that holds the input files for the jobs.")])]),a._v(" "),t("h3",{attrs:{id:"updating-the-job-to-run-on-pyspark"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#updating-the-job-to-run-on-pyspark"}},[a._v("#")]),a._v(" Updating the job to run on pyspark")]),a._v(" "),t("p",[a._v("To update our code to run on "),t("code",[a._v("pyspark")]),a._v(" instead of "),t("code",[a._v("nodejs")]),a._v(", we will update the source yaml of our job.")]),a._v(" "),t("p",[a._v("Open "),t("code",[a._v("src/pipelines/customer/sample.yaml")]),a._v(":")]),a._v(" "),t("p",[a._v("On line 4, notice that "),t("code",[a._v("runs_on")]),a._v(" determines the runner to use for the job:")]),a._v(" "),t("div",{staticClass:"language-yaml extra-class"},[t("pre",{pre:!0,attrs:{class:"language-yaml"}},[t("code",[t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("jobs")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("customer-sample")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("description")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" this is a basic job to load customer data from CSV file into a table\n    "),t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("runs_on")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" nodejs\n")])])]),t("p",[a._v("Change the "),t("code",[a._v("runs_on")]),a._v(" key to indicate "),t("code",[a._v("pyspark")]),a._v(". Modified 4 lines should appear as:")]),a._v(" "),t("div",{staticClass:"language-yaml extra-class"},[t("pre",{pre:!0,attrs:{class:"language-yaml"}},[t("code",[t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("jobs")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("customer-sample")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("description")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" this is a basic job to load customer data from CSV file into a table\n    "),t("span",{pre:!0,attrs:{class:"token key atrule"}},[a._v("runs_on")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v(" pyspark\n")])])]),t("h3",{attrs:{id:"validating-the-install-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#validating-the-install-2"}},[a._v("#")]),a._v(" Validating the install")]),a._v(" "),t("p",[a._v("Let's re-run the sample job:")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("dy-cli run sample.customer\n")])])]),t("p",[a._v("If all goes well, you should see some startup logs, and eventually:")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("+-----+-----+\n|   id| name|\n+-----+-----+\n|hello|world|\n+-----+-----+\n")])])]),t("p",[a._v("That's it! You've created your first job that loads data from CSV, runs it through Spark, and shows the data to the standard output.")]),a._v(" "),t("p",[a._v("Read on for a more detailed tutorial or check out the reference to see the different block types currently available.")])])}),[],!1,null,null,null);e.default=r.exports}}]);