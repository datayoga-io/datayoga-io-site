(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{390:function(s,t,a){"use strict";a.r(t);var e=a(46),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"job"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#job"}},[s._v("#")]),s._v(" Job")]),s._v(" "),a("p",[s._v("representation of an ETL job")]),s._v(" "),a("h2",{attrs:{id:"runs-on"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#runs-on"}},[s._v("#")]),s._v(" runs-on")]),s._v(" "),a("ul",[a("li",[s._v("Type: "),a("code",[s._v('"pyspark" | "pysql" | "jssql"')])]),s._v(" "),a("li",[s._v("Description: Which runner to use to process this pipeline. "),a("code",[s._v("pyspark")]),s._v(" uses pyspark with the dataframe API. "),a("code",[s._v("pysql")]),s._v(" uses Python and SQLAlchemy and is suited to an ELT approach. "),a("code",[s._v("jssql")]),s._v(" uses Javascript and TypeORM and is suited to an ELT approach")])]),s._v(" "),a("p",[s._v("undefined")]),s._v(" "),a("h2",{attrs:{id:"steps"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#steps"}},[s._v("#")]),s._v(" steps")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("Type: "),a("code",[s._v("array")])])]),s._v(" "),a("li",[a("p",[s._v("Description: List of blocks. These will be run in order, where each step's output is the next step's input, unless the 'inputs' clause is specified")])]),s._v(" "),a("li",[a("p",[s._v("Examples:")])])]),s._v(" "),a("div",{staticClass:"language-yaml extra-class"},[a("pre",{pre:!0,attrs:{class:"language-yaml"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("-")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("id")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" extract_csv\n  "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("uses")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" extract\n  "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("properties")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("source")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" employee\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("-")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("id")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" load_snowflake\n  "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("uses")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" load\n  "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("properties")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("table_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" tab\n  "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("inputs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token key atrule"}},[s._v("df")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" extract/df\n\n")])])])])}),[],!1,null,null,null);t.default=n.exports}}]);